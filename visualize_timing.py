#!/usr/bin/env python3
"""
4DGSè®­ç»ƒæ—¶é—´å¯è§†åŒ–åˆ†æå·¥å…·
åˆ†ææ¯è½®ç”¨æ—¶æ›²çº¿å’Œå„éƒ¨åˆ†çš„å æ¯”
"""

import json
import matplotlib.pyplot as plt
import numpy as np
from pathlib import Path
import argparse
import sys

# å°è¯•å¯¼å…¥seabornï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨matplotlib
try:
    import seaborn as sns
    HAS_SEABORN = True
except ImportError:
    HAS_SEABORN = False
    print("âš ï¸  seabornæœªå®‰è£…ï¼Œå°†ä½¿ç”¨matplotlibç»˜åˆ¶çƒ­åŠ›å›¾")

# è®¾ç½®å­—ä½“ï¼ˆå°è¯•ä¸­æ–‡å­—ä½“ï¼Œå¤±è´¥åˆ™ä½¿ç”¨è‹±æ–‡ï¼‰
try:
    plt.rcParams['font.sans-serif'] = ['WenQuanYi Micro Hei', 'SimHei', 'DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False
except:
    plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False

class TimingVisualizer:
    def __init__(self, timing_report_path):
        """åˆå§‹åŒ–å¯è§†åŒ–å™¨"""
        self.timing_report_path = Path(timing_report_path)
        self.data = None
        self.load_data()
        
    def load_data(self):
        """åŠ è½½æ—¶é—´æŠ¥å‘Šæ•°æ®"""
        try:
            with open(self.timing_report_path, 'r', encoding='utf-8') as f:
                self.data = json.load(f)
            print(f"âœ… æˆåŠŸåŠ è½½æ—¶é—´æŠ¥å‘Š: {self.timing_report_path}")
        except Exception as e:
            print(f"âŒ åŠ è½½æ—¶é—´æŠ¥å‘Šå¤±è´¥: {e}")
            sys.exit(1)
    
    def extract_iteration_data(self):
        """æå–æ¯è½®è¿­ä»£æ•°æ®ï¼ˆè°ƒæ•´fineé˜¶æ®µçš„iterationç¼–å·ï¼Œä½¿å…¶è¿ç»­ï¼‰"""
        per_iteration = self.data.get('per_iteration_timings', {})
        training_logs = self.data.get('training_logs', [])
        
        # ç®€åŒ–é€»è¾‘ï¼šæ ¹æ®training_logsåˆ¤æ–­stageèŒƒå›´
        coarse_iters_set = set()
        fine_iters_set = set()
        
        for log in training_logs:
            iter_num = log.get('iteration')
            stage = log.get('stage')
            if iter_num and stage:
                if stage == 'coarse':
                    coarse_iters_set.add(iter_num)
                elif stage == 'fine':
                    fine_iters_set.add(iter_num)
        
        # Coarseé˜¶æ®µçš„æœ€å¤§iteration
        coarse_max_iter = max(coarse_iters_set) if coarse_iters_set else 3000
        fine_max_iter = max(fine_iters_set) if fine_iters_set else 14000
        
        iterations = []
        total_times = []
        stages = []
        losses = []
        psnrs = []
        actual_iterations = []  # å®é™…çš„iterationç¼–å·ï¼ˆç”¨äºæŸ¥æ‰¾logï¼‰
        
        # éå†per_iteration_timingsï¼Œæå–æ•°æ®
        for iter_num_str in sorted(per_iteration.keys(), key=lambda x: int(x)):
            timings = per_iteration[iter_num_str]
            if isinstance(timings, dict):
                actual_iter = int(iter_num_str)
                
                # ç®€å•åˆ¤æ–­ï¼š
                # Coarse: iteration 1-3000ï¼Œä½¿ç”¨coarse_total
                # Fine: iteration 1-14000ï¼Œä½¿ç”¨fine_totalï¼Œè°ƒæ•´ä¸º3001-17000
                if actual_iter <= coarse_max_iter:
                    # Coarseé˜¶æ®µ
                    if 'coarse_total' in timings and timings['coarse_total'] > 0:
                        iterations.append(actual_iter)
                        actual_iterations.append(actual_iter)
                        total_times.append(timings['coarse_total'])
                        stages.append('coarse')
                        
                if actual_iter <= fine_max_iter:
                    # Fineé˜¶æ®µï¼ˆiteration 1-14000æ˜ å°„åˆ°3001-17000ï¼‰
                    if 'fine_total' in timings and timings['fine_total'] > 0:
                        adjusted_iter = actual_iter + coarse_max_iter
                        iterations.append(adjusted_iter)
                        actual_iterations.append(actual_iter)
                        total_times.append(timings['fine_total'])
                        stages.append('fine')
        
        # æå–è®­ç»ƒæŒ‡æ ‡ï¼ˆåªæå–æœ‰è®°å½•çš„iterationï¼Œé¿å…å¡«å……0å€¼ï¼‰
        # training_logsä¸­åªè®°å½•äº†æ¯10æ­¥ï¼Œå…¶ä»–iterationä¸åº”è¯¥æ˜¾ç¤º
        for i, actual_iter in enumerate(actual_iterations):
            stage = stages[i]
            # åœ¨training_logsä¸­æ‰¾åˆ°å¯¹åº”çš„è®°å½•
            found = False
            for log in training_logs:
                if log.get('iteration') == actual_iter and log.get('stage') == stage:
                    losses.append(log.get('loss', None))
                    psnrs.append(log.get('psnr', None))
                    found = True
                    break
            if not found:
                # ä½¿ç”¨Noneè€Œä¸æ˜¯0ï¼Œè¿™æ ·ç»˜å›¾æ—¶ä¼šè‡ªåŠ¨è·³è¿‡
                losses.append(None)
                psnrs.append(None)
        
        return {
            'iterations': iterations,
            'total_times': total_times,
            'stages': stages,
            'losses': losses,
            'psnrs': psnrs
        }
    
    def extract_operation_data(self):
        """æå–å„æ“ä½œæ•°æ®ï¼ˆè°ƒæ•´fineé˜¶æ®µçš„iterationç¼–å·ï¼Œä½¿å…¶è¿ç»­ï¼‰"""
        per_iteration = self.data.get('per_iteration_timings', {})
        training_logs = self.data.get('training_logs', [])
        
        # æ‰¾åˆ°coarseå’Œfineé˜¶æ®µçš„æœ€å¤§iteration
        coarse_iters_in_log = set()
        fine_iters_in_log = set()
        
        for log in training_logs:
            iter_num = log.get('iteration')
            stage = log.get('stage')
            if iter_num and stage:
                if stage == 'coarse':
                    coarse_iters_in_log.add(iter_num)
                elif stage == 'fine':
                    fine_iters_in_log.add(iter_num)
        
        coarse_max_iter = max(coarse_iters_in_log) if coarse_iters_in_log else 3000
        fine_max_iter = max(fine_iters_in_log) if fine_iters_in_log else 14000
        
        # æ”¶é›†æ‰€æœ‰æ“ä½œåç§°ï¼Œæ’é™¤iterationï¼ˆå› ä¸ºå®ƒæ˜¯æ€»æ—¶é—´ï¼Œä¼šå¯¼è‡´é‡å¤è®¡ç®—ï¼‰
        all_operations = set()
        for timings in per_iteration.values():
            if isinstance(timings, dict):
                for key in timings.keys():
                    if not key.endswith('_total') and not key.endswith('_iteration'):
                        all_operations.add(key)
        
        # æŒ‰é˜¶æ®µåˆ†ç»„æ“ä½œ
        coarse_ops = [op for op in all_operations if op.startswith('coarse_')]
        fine_ops = [op for op in all_operations if op.startswith('fine_')]
        
        # æå–æ•°æ®ï¼ˆå’Œextract_iteration_dataé€»è¾‘ä¸€è‡´ï¼‰
        # å…³é”®ï¼šæ¯ä¸ªtimingsåŒæ—¶åŒ…å«coarseå’Œfineçš„æ‰€æœ‰æ“ä½œï¼Œéœ€è¦åˆ†åˆ«æå–
        
        iterations = []
        operation_data = {}
        for op in all_operations:
            operation_data[op] = []
        
        for iter_num_str in sorted(per_iteration.keys(), key=lambda x: int(x)):
            timings = per_iteration[iter_num_str]
            if isinstance(timings, dict):
                actual_iter = int(iter_num_str)
                
                # Coarse: iteration 1-3000
                if actual_iter <= coarse_max_iter:
                    iterations.append(actual_iter)
                    # åªæå–coarseæ“ä½œçš„æ•°æ®
                    for op in all_operations:
                        operation_data[op].append(timings.get(op, 0))
                
                # Fine: iteration 1-14000ï¼Œè°ƒæ•´ä¸º3001-17000
                if actual_iter <= fine_max_iter:
                    adjusted_iter = actual_iter + coarse_max_iter
                    iterations.append(adjusted_iter)
                    # åªæå–fineæ“ä½œçš„æ•°æ®ï¼ˆæ³¨æ„ï¼šcoarseæ“ä½œä¼šæ˜¯0ï¼‰
                for op in all_operations:
                    operation_data[op].append(timings.get(op, 0))
        
        return {
            'iterations': iterations,
            'operations': operation_data,
            'coarse_ops': coarse_ops,
            'fine_ops': fine_ops,
            'coarse_max_iter': coarse_max_iter  # è¿”å›coarseæœ€å¤§iterationï¼Œä¾›ç»˜å›¾ä½¿ç”¨
        }
    
    def plot_iteration_timing_curve(self, save_path=None, save_path_full=None):
        """ç»˜åˆ¶æ¯è½®ç”¨æ—¶æ›²çº¿ï¼ˆç”Ÿæˆä¸¤ä¸ªç‰ˆæœ¬ï¼š98åˆ†ä½æ•°ç‰ˆæœ¬ + æ–­è½´å®Œæ•´ç‰ˆæœ¬ï¼‰"""
        data = self.extract_iteration_data()
        
        # æŒ‰é˜¶æ®µåˆ†ç»„æ•°æ®
        coarse_data = {'iterations': [], 'times': [], 'losses': [], 'psnrs': []}
        fine_data = {'iterations': [], 'times': [], 'losses': [], 'psnrs': []}
        
        for i, stage in enumerate(data['stages']):
            if stage == 'coarse':
                coarse_data['iterations'].append(data['iterations'][i])
                coarse_data['times'].append(data['total_times'][i])
                coarse_data['losses'].append(data['losses'][i])
                coarse_data['psnrs'].append(data['psnrs'][i])
            elif stage == 'fine':
                fine_data['iterations'].append(data['iterations'][i])
                fine_data['times'].append(data['total_times'][i])
                fine_data['losses'].append(data['losses'][i])
                fine_data['psnrs'].append(data['psnrs'][i])
        
        # ========== ç‰ˆæœ¬1ï¼š98åˆ†ä½æ•°è§†å›¾ ==========
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10))
        
        # ç»˜åˆ¶ç”¨æ—¶æ›²çº¿ - å¤„ç†å¼‚å¸¸å€¼ï¼Œä½¿ç”¨æ•£ç‚¹å›¾
        if coarse_data['iterations']:
            ax1.scatter(coarse_data['iterations'], coarse_data['times'], 
                       label='Coarse Stage', color='blue', alpha=0.6, s=20, edgecolors='none')
            
        if fine_data['iterations']:
            ax1.scatter(fine_data['iterations'], fine_data['times'], 
                       label='Fine Stage', color='red', alpha=0.6, s=20, edgecolors='none')
        
        # è®¾ç½®åˆç†çš„yè½´ä¸Šé™å’Œä¸‹é™ï¼ˆé¿å…æç«¯å€¼å‹ç¼©æ•°æ®ï¼Œå‡å°‘ç©ºç™½åŒºåŸŸï¼‰
        all_times = coarse_data['times'] + fine_data['times']
        if all_times:
            y_min = np.percentile(all_times, 5) * 0.9  # ä½¿ç”¨5åˆ†ä½æ•°çš„90%ä½œä¸ºä¸‹é™
            y_max = np.percentile(all_times, 98)  # ä½¿ç”¨98åˆ†ä½æ•°ä½œä¸ºä¸Šé™
            outliers_count = np.sum(np.array(all_times) > y_max)
            max_val = np.max(all_times)
            ax1.set_ylim(y_min, y_max * 1.1)
            
            # # æ·»åŠ ç¦»ç¾¤ç‚¹æç¤º
            # if outliers_count > 0:
            #     ax1.text(0.98, 0.98, f'âš ï¸ {outliers_count} outliers\n(max: {max_val:.3f}s)', 
            #            transform=ax1.transAxes, fontsize=9, 
            #            verticalalignment='top', horizontalalignment='right',
            #            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
        
        ax1.set_xlabel('Iteration (Coarse: 1-3000, Fine: 3001-17000)', fontsize=11)
        ax1.set_ylabel('Time per Iteration (seconds)', fontsize=11)
        ax1.set_title('Training Time per Iteration ', fontsize=13, fontweight='bold')
        ax1.legend(fontsize=10, loc='upper left')
        ax1.grid(True, alpha=0.3)
        
        # ç»˜åˆ¶æŸå¤±å’ŒPSNRæ›²çº¿ - ä½¿ç”¨æŠ˜çº¿å›¾ï¼ˆè¿‡æ»¤Noneå€¼ï¼‰
        ax2_twin = ax2.twinx()
        
        if coarse_data['iterations']:
            # è¿‡æ»¤Noneå€¼
            valid_coarse = [(it, loss, psnr) for it, loss, psnr in 
                           zip(coarse_data['iterations'], coarse_data['losses'], coarse_data['psnrs'])
                           if loss is not None and psnr is not None]
            if valid_coarse:
                coarse_its, coarse_losses, coarse_psnrs = zip(*valid_coarse)
                ax2.plot(coarse_its, coarse_losses, 
                        label='Coarse Loss', color='blue', alpha=0.8, linewidth=2)
                ax2_twin.plot(coarse_its, coarse_psnrs, 
                             label='Coarse PSNR', color='lightblue', alpha=0.8, 
                             linewidth=2)
        
        if fine_data['iterations']:
            # è¿‡æ»¤Noneå€¼
            valid_fine = [(it, loss, psnr) for it, loss, psnr in 
                         zip(fine_data['iterations'], fine_data['losses'], fine_data['psnrs'])
                         if loss is not None and psnr is not None]
            if valid_fine:
                fine_its, fine_losses, fine_psnrs = zip(*valid_fine)
                ax2.plot(fine_its, fine_losses, 
                        label='Fine Loss', color='red', alpha=0.8, linewidth=2)
                ax2_twin.plot(fine_its, fine_psnrs, 
                             label='Fine PSNR', color='pink', alpha=0.8, 
                             linewidth=2)
        
        # ä¼˜åŒ–lossçš„yè½´èŒƒå›´ï¼Œä½¿å¯¹æ¯”æ›´æ˜æ˜¾ï¼ˆå»æ‰æœ€å¼€å§‹çš„é«˜losså€¼ï¼‰
        all_losses = [l for l in coarse_data['losses'] + fine_data['losses'] if l is not None and l > 0]
        if all_losses:
            loss_min = min(all_losses)
            loss_max = max(all_losses)
            # ä½¿ç”¨10åˆ†ä½æ•°ä½œä¸ºä¸‹é™ï¼Œå»æ‰æœ€å¼€å§‹çš„æé«˜å€¼ï¼Œçªå‡ºåæœŸå˜åŒ–
            loss_p10 = np.percentile(all_losses, 10)
            loss_p95 = np.percentile(all_losses, 95)
            # è®¾ç½®ç´§å‡‘çš„èŒƒå›´
            ax2.set_ylim(0, loss_p95 * 1.2)
        
        ax2.set_xlabel('Iteration (Coarse: 1-3000, Fine: 3001-17000)', fontsize=11)
        ax2.set_ylabel('Loss', color='black', fontsize=11)
        ax2_twin.set_ylabel('PSNR (dB)', color='gray', fontsize=11)
        ax2.set_title('Training Metrics (Loss & PSNR)', fontsize=13, fontweight='bold')
        ax2.legend(loc='upper left', fontsize=10)
        ax2_twin.legend(loc='upper right', fontsize=10)
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š ç”¨æ—¶æ›²çº¿å›¾ï¼ˆ98åˆ†ä½æ•°ï¼‰å·²ä¿å­˜: {save_path}")
        plt.show()
        plt.close()
        
        # ========== ç‰ˆæœ¬2ï¼šæ–­è½´å®Œæ•´è§†å›¾ï¼ˆæ˜¾ç¤ºæ‰€æœ‰æç«¯å€¼ï¼‰ ==========
        if save_path_full and all_times:
            self._plot_iteration_timing_curve_broken_axis(coarse_data, fine_data, all_times, save_path_full)
    
    def _plot_iteration_timing_curve_broken_axis(self, coarse_data, fine_data, all_times, save_path):
        """ç»˜åˆ¶å¸¦æ–­è½´çš„å®Œæ•´ç”¨æ—¶æ›²çº¿ï¼ˆæ˜¾ç¤ºæ‰€æœ‰æç«¯å€¼ï¼‰"""
        from matplotlib.gridspec import GridSpec
        
        all_times_array = np.array(all_times)
        p90 = np.percentile(all_times_array, 90)
        p98 = np.percentile(all_times_array, 98)
        max_val = np.max(all_times_array)
        
        # å¦‚æœæœ€å¤§å€¼è¿œå¤§äº90åˆ†ä½æ•°ï¼Œä½¿ç”¨æ–­è½´
        if max_val > p90 * 1.5:
            fig = plt.figure(figsize=(14, 12))
            
            # æ‰‹åŠ¨è®¾ç½®å­å›¾ä½ç½®ï¼ŒæŒ‰1:3:2æ¯”ä¾‹ï¼Œæ–­è½´ç©ºéš™æå°
            gap_break = 0.005  # ä¸Šä¸­å›¾ä¹‹é—´æå°ï¼ˆæ–­è½´ç©ºéš™ï¼‰
            gap_metrics = 0.10  # ä¸­ä¸‹å›¾ä¹‹é—´è¾ƒå¤§ï¼ˆè®­ç»ƒæŒ‡æ ‡ç©ºéš™ï¼‰- å¢å¤§
            
            # ç»Ÿä¸€çš„å·¦å³è¾¹è·ï¼ˆä¸‹å›¾æœ‰ä¸¤ä¸ªyè½´ï¼Œéœ€è¦æ›´å¤šå³è¾¹è·ï¼‰
            left_margin = 0.10
            right_margin = 0.12  # ä¸ºtwin axisç•™å‡ºç©ºé—´
            plot_width = 1.0 - left_margin - right_margin  # 0.78
            
            # è®¡ç®—é«˜åº¦ï¼ˆæ€»å¯ç”¨ç©ºé—´ 0.05-0.93 = 0.88ï¼Œå‡å»ç©ºéš™åæŒ‰1:3:2åˆ†é…ï¼‰
            total_height = 0.88 - gap_break - gap_metrics  # 0.765
            h_top = total_height * 1/6  # çº¦0.127
            h_middle = total_height * 3/6  # çº¦0.382
            h_bottom = total_height * 2/6  # çº¦0.255
            
            # ä¸‰ä¸ªå›¾éƒ½ä½¿ç”¨ç›¸åŒçš„leftå’Œwidthï¼Œç¡®ä¿å·¦å³è¾¹ç•Œå¯¹é½
            ax_top = fig.add_subplot(3, 1, 1)
            ax_top.set_position([left_margin, 0.93-h_top, plot_width, h_top])
            
            ax_bottom = fig.add_subplot(3, 1, 2)
            ax_bottom.set_position([left_margin, 0.93-h_top-gap_break-h_middle, plot_width, h_middle])
            
            ax_metrics = fig.add_subplot(3, 1, 3)
            ax_metrics.set_position([left_margin, 0.05, plot_width, h_bottom])
            
            # è®¾ç½®yè½´èŒƒå›´
            y_bottom_min = np.percentile(all_times_array, 5) * 0.9  # ä½¿ç”¨5åˆ†ä½æ•°çš„90%ä½œä¸ºä¸‹é™ï¼Œå‡å°‘ç©ºç™½
            y_bottom_max = p90 * 1.2
            y_top_min = p98 * 0.95
            
            # åœ¨ä¸¤ä¸ªå­å›¾ä¸­ç»˜åˆ¶ç›¸åŒçš„æ•°æ®
            for ax in [ax_top, ax_bottom]:
                if coarse_data['iterations']:
                    ax.scatter(coarse_data['iterations'], coarse_data['times'], 
                              label='Coarse' if ax == ax_bottom else None,
                              color='blue', alpha=0.6, s=15, edgecolors='none')
                if fine_data['iterations']:
                    ax.scatter(fine_data['iterations'], fine_data['times'], 
                              label='Fine' if ax == ax_bottom else None,
                              color='red', alpha=0.6, s=15, edgecolors='none')
            
            # è®¾ç½®yè½´èŒƒå›´
            ax_bottom.set_ylim(y_bottom_min, y_bottom_max)
            ax_top.set_ylim(y_top_min, max_val * 1.05)
            
            # éšè—ä¸Šå›¾xè½´
            ax_top.set_xticklabels([])
            ax_top.tick_params(labelbottom=False, bottom=False)
            
            # æ·»åŠ æ–­è½´æ ‡è®°
            d = 0.015
            kwargs = dict(transform=ax_top.transAxes, color='k', clip_on=False, linewidth=1.5)
            ax_top.plot((-d, +d), (-d, +d), **kwargs)
            ax_top.plot((1 - d, 1 + d), (-d, +d), **kwargs)
            
            kwargs.update(transform=ax_bottom.transAxes)
            ax_bottom.plot((-d, +d), (1 - d, 1 + d), **kwargs)
            ax_bottom.plot((1 - d, 1 + d), (1 - d, 1 + d), **kwargs)
            
            # è®¾ç½®æ ‡ç­¾
            ax_bottom.set_xlabel('Iteration (Coarse: 1-3000, Fine: 3001-17000)', fontsize=11)
            ax_bottom.set_ylabel('Time (seconds)', fontsize=10)
            ax_top.set_ylabel('Time (sec)', fontsize=10)
            ax_top.set_title('Training Time per Iteration (Full View with Broken Axis)', 
                           fontsize=13, fontweight='bold', pad=15)
            ax_bottom.legend(fontsize=10, loc='upper left')
            ax_bottom.grid(True, alpha=0.3)
            ax_top.grid(True, alpha=0.3)
            
            # ç»˜åˆ¶è®­ç»ƒæŒ‡æ ‡ - ä½¿ç”¨æŠ˜çº¿å›¾ï¼ˆè¿‡æ»¤Noneå€¼ï¼‰
            ax_metrics_twin = ax_metrics.twinx()
            
            if coarse_data['iterations']:
                valid_coarse_metrics = [(it, loss, psnr) for it, loss, psnr in 
                                       zip(coarse_data['iterations'], coarse_data['losses'], coarse_data['psnrs'])
                                       if loss is not None and psnr is not None]
                if valid_coarse_metrics:
                    c_its, c_losses, c_psnrs = zip(*valid_coarse_metrics)
                    ax_metrics.plot(c_its, c_losses, 
                                   label='Coarse Loss', color='blue', alpha=0.8, linewidth=2)
                    ax_metrics_twin.plot(c_its, c_psnrs, 
                                        label='Coarse PSNR', color='lightblue', alpha=0.8, 
                                        linewidth=2)
            
            if fine_data['iterations']:
                valid_fine_metrics = [(it, loss, psnr) for it, loss, psnr in 
                                     zip(fine_data['iterations'], fine_data['losses'], fine_data['psnrs'])
                                     if loss is not None and psnr is not None]
                if valid_fine_metrics:
                    f_its, f_losses, f_psnrs = zip(*valid_fine_metrics)
                    ax_metrics.plot(f_its, f_losses, 
                                   label='Fine Loss', color='red', alpha=0.8, linewidth=2)
                    ax_metrics_twin.plot(f_its, f_psnrs, 
                                        label='Fine PSNR', color='pink', alpha=0.8, 
                                        linewidth=2)
            
            # ä¼˜åŒ–lossçš„yè½´èŒƒå›´ï¼Œä½¿å¯¹æ¯”æ›´æ˜æ˜¾ï¼ˆå»æ‰æœ€å¼€å§‹çš„é«˜losså€¼ï¼‰
            all_losses_full = [l for l in coarse_data['losses'] + fine_data['losses'] if l is not None and l > 0]
            if all_losses_full:
                loss_p95_full = np.percentile(all_losses_full, 95)
                # ä»0å¼€å§‹ï¼Œåˆ°95åˆ†ä½æ•°ï¼Œè‡ªåŠ¨è£å‰ªåˆæœŸçš„æé«˜losså€¼
                ax_metrics.set_ylim(0, loss_p95_full * 1.2)
            
            ax_metrics.set_xlabel('Iteration', fontsize=11)
            ax_metrics.set_ylabel('Loss', fontsize=10)
            ax_metrics_twin.set_ylabel('PSNR (dB)', fontsize=10)
            ax_metrics.set_title('Training Metrics (Loss & PSNR)', fontsize=13, fontweight='bold')
            ax_metrics.legend(loc='upper left', fontsize=9)
            ax_metrics_twin.legend(loc='upper right', fontsize=9)
            ax_metrics.grid(True, alpha=0.3)
            
            # ä¸ä½¿ç”¨tight_layoutï¼Œå› ä¸ºæˆ‘ä»¬å·²ç»æ‰‹åŠ¨è®¾ç½®äº†position
            # plt.tight_layout()  # ä¼šè¦†ç›–æ‰‹åŠ¨è®¾ç½®çš„position
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š ç”¨æ—¶æ›²çº¿å›¾ï¼ˆæ–­è½´å®Œæ•´ç‰ˆï¼‰å·²ä¿å­˜: {save_path}")
            plt.show()
            plt.close()
        else:
            print("âš ï¸  æ•°æ®æ²¡æœ‰æ˜æ˜¾æç«¯å€¼ï¼Œæ— éœ€æ–­è½´æ˜¾ç¤º")
    
    def plot_stage_time_comparison(self, save_path=None):
        """ç»˜åˆ¶Coarse vs Fineé˜¶æ®µæ€»æ—¶é—´å æ¯”"""
        data = self.extract_iteration_data()
        
        # è®¡ç®—å„é˜¶æ®µæ€»æ—¶é—´
        coarse_total_time = 0
        fine_total_time = 0
        
        for i, stage in enumerate(data['stages']):
            if stage == 'coarse':
                coarse_total_time += data['total_times'][i]
            elif stage == 'fine':
                fine_total_time += data['total_times'][i]
        
        total_time = coarse_total_time + fine_total_time
        
        # ç»˜åˆ¶é¥¼å›¾
        fig, ax = plt.subplots(1, 1, figsize=(10, 8))
        
        stages = ['Coarse Stage', 'Fine Stage']
        times = [coarse_total_time, fine_total_time]
        percentages = [t/total_time*100 for t in times]
        colors = ['#3498db', '#e74c3c']
        
        wedges, texts, autotexts = ax.pie(times, labels=stages, autopct='%1.1f%%',
                                          colors=colors, startangle=90,
                                          textprops={'fontsize': 13, 'fontweight': 'bold'},
                                          pctdistance=0.75, labeldistance=1.1)
        
        # è®¾ç½®æ ‡é¢˜å’Œå›¾ä¾‹
        ax.set_title('Training Time: Coarse vs Fine Stage Comparison', 
                    fontsize=14, fontweight='bold', pad=20)
        
        # æ·»åŠ è¯¦ç»†ä¿¡æ¯æ–‡æœ¬æ¡†
        info_text = f'Coarse: {coarse_total_time:.1f}s ({percentages[0]:.1f}%)\n'
        info_text += f'Fine: {fine_total_time:.1f}s ({percentages[1]:.1f}%)\n'
        info_text += f'Total: {total_time:.1f}s'
        
        ax.text(0.02, 0.98, info_text, transform=ax.transAxes,
               fontsize=11, verticalalignment='top',
               bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š é˜¶æ®µæ—¶é—´å¯¹æ¯”å›¾å·²ä¿å­˜: {save_path}")
        
        plt.show()
        plt.close()
    
    def plot_operation_breakdown(self, save_path=None):
        """ç»˜åˆ¶æ“ä½œå æ¯”åˆ†æï¼ˆé¥¼å›¾ä¸­æ ‡æ³¨åºå·ï¼Œå›¾ä¾‹æ˜¾ç¤ºåºå·å¯¹åº”çš„æ“ä½œåç§°ï¼‰"""
        data = self.extract_operation_data()
        
        # è®¡ç®—å¹³å‡å æ¯”
        avg_percentages = {}
        for op, times in data['operations'].items():
            if times and any(t > 0 for t in times):
                avg_time = np.mean([t for t in times if t > 0])
                avg_percentages[op] = avg_time
        
        # æŒ‰é˜¶æ®µåˆ†ç»„å¹¶æŒ‰æ—¶é—´æ’åº
        coarse_avg = {op: avg_percentages[op] for op in data['coarse_ops'] if op in avg_percentages}
        fine_avg = {op: avg_percentages[op] for op in data['fine_ops'] if op in avg_percentages}
        
        # æ’åºï¼ˆä»å¤§åˆ°å°ï¼‰
        coarse_avg = dict(sorted(coarse_avg.items(), key=lambda x: x[1], reverse=True))
        fine_avg = dict(sorted(fine_avg.items(), key=lambda x: x[1], reverse=True))
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))
        
        # Coarseé˜¶æ®µå æ¯”
        if coarse_avg:
            ops = [op.replace('coarse_', '').replace('_', ' ').title() for op in coarse_avg.keys()]
            times = list(coarse_avg.values())
            colors = plt.cm.Set3(np.linspace(0, 1, len(ops)))
            
            # è®¡ç®—ç™¾åˆ†æ¯”ï¼Œåªåœ¨>=3%çš„æ‰‡åŒºæ˜¾ç¤ºåºå·æ ‡ç­¾ï¼Œé¿å…é‡å 
            total_time = sum(times)
            percentages = [t/total_time*100 for t in times]
            labels = [str(i+1) if percentages[i] >= 3 else '' for i in range(len(ops))]
            
            # è‡ªå®šä¹‰ç™¾åˆ†æ¯”æ˜¾ç¤ºå‡½æ•°
            def make_autopct(values):
                def my_autopct(pct):
                    return f'{pct:.1f}%' if pct >= 3 else ''
                return my_autopct
            
            wedges, texts, autotexts = ax1.pie(times, labels=labels, autopct=make_autopct(percentages), 
                                             colors=colors, startangle=90, 
                                             textprops={'fontsize': 10, 'fontweight': 'bold'},
                                             pctdistance=0.75, labeldistance=1.08)
            
            # å›¾ä¾‹æ˜¾ç¤ºåºå·å¯¹åº”çš„æ“ä½œåç§°ï¼ˆæŒ‰å æ¯”ä»å¤§åˆ°å°ï¼‰
            legend_labels = [f'{i+1}. {op} ({percentages[i]:.1f}%)' for i, op in enumerate(ops)]
            ax1.legend(legend_labels, loc='center left', bbox_to_anchor=(1.05, 0.5), 
                      fontsize=9, frameon=True, shadow=True)
            ax1.set_title('Coarse Stage Operation Time Distribution', fontsize=13, fontweight='bold', pad=20)
        
        # Fineé˜¶æ®µå æ¯”
        if fine_avg:
            ops = [op.replace('fine_', '').replace('_', ' ').title() for op in fine_avg.keys()]
            times = list(fine_avg.values())
            colors = plt.cm.Set3(np.linspace(0, 1, len(ops)))
            
            # è®¡ç®—ç™¾åˆ†æ¯”ï¼Œåªåœ¨>=3%çš„æ‰‡åŒºæ˜¾ç¤ºåºå·æ ‡ç­¾ï¼Œé¿å…é‡å 
            total_time = sum(times)
            percentages = [t/total_time*100 for t in times]
            labels = [str(i+1) if percentages[i] >= 3 else '' for i in range(len(ops))]
            
            def make_autopct(values):
                def my_autopct(pct):
                    return f'{pct:.1f}%' if pct >= 3 else ''
                return my_autopct
            
            wedges, texts, autotexts = ax2.pie(times, labels=labels, autopct=make_autopct(percentages), 
                                             colors=colors, startangle=90,
                                             textprops={'fontsize': 10, 'fontweight': 'bold'},
                                             pctdistance=0.75, labeldistance=1.08)
            
            legend_labels = [f'{i+1}. {op} ({percentages[i]:.1f}%)' for i, op in enumerate(ops)]
            ax2.legend(legend_labels, loc='center left', bbox_to_anchor=(1.05, 0.5), 
                      fontsize=9, frameon=True, shadow=True)
            ax2.set_title('Fine Stage Operation Time Distribution', fontsize=13, fontweight='bold', pad=20)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š æ“ä½œå æ¯”å›¾å·²ä¿å­˜: {save_path}")
        
        plt.show()
    
    def plot_operation_trends(self, save_path=None):
        """ç»˜åˆ¶å„æ“ä½œç”¨æ—¶è¶‹åŠ¿ï¼ˆæ¨ªè½´è¿ç»­ï¼šCoarseåœ¨å‰ï¼ŒFineåœ¨åï¼‰"""
        data = self.extract_operation_data()
        coarse_max_iter = data.get('coarse_max_iter', 3000)
        
        # é€‰æ‹©ä¸»è¦æ“ä½œè¿›è¡Œå¯è§†åŒ–
        main_operations = ['data_loading', 'render', 'loss_computation', 'optimizer_step']
        
        fig, axes = plt.subplots(2, 2, figsize=(16, 10))
        axes = axes.flatten()
        
        for i, op in enumerate(main_operations):
            if i >= len(axes):
                break
                
            ax = axes[i]
            
            # æ”¶é›†coarseå’Œfineçš„æ•°æ®ï¼ˆæ ¹æ®è°ƒæ•´åçš„iterationåˆ†ç¦»ï¼‰
            coarse_op = f'coarse_{op}'
            fine_op = f'fine_{op}'
            
            all_times = []
            coarse_data = {'iters': [], 'times': []}
            fine_data = {'iters': [], 'times': []}
            
            # å¤„ç†coarseæ•°æ®ï¼ˆiteration <= coarse_max_iterï¼‰
            if coarse_op in data['operations']:
                coarse_times = data['operations'][coarse_op]
                for iter_num, time in zip(data['iterations'], coarse_times):
                    if time > 0 and iter_num <= coarse_max_iter:
                        coarse_data['iters'].append(iter_num)
                        coarse_data['times'].append(time)
                        all_times.append(time)
            
            # å¤„ç†fineæ•°æ®ï¼ˆiteration > coarse_max_iterï¼‰
            if fine_op in data['operations']:
                fine_times = data['operations'][fine_op]
                for iter_num, time in zip(data['iterations'], fine_times):
                    if time > 0 and iter_num > coarse_max_iter:
                        fine_data['iters'].append(iter_num)
                        fine_data['times'].append(time)
                        all_times.append(time)
            
            if not all_times:
                continue
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            all_times_array = np.array(all_times)
            p95 = np.percentile(all_times_array, 95)
            max_val = np.max(all_times_array)
            median_val = np.median(all_times_array)
            
            # ç»˜åˆ¶æ•°æ®ï¼ˆä½¿ç”¨æ•£ç‚¹å›¾ï¼‰
            if coarse_data['iters']:
                ax.scatter(coarse_data['iters'], coarse_data['times'], 
                          label='Coarse', color='blue', alpha=0.6, s=15, edgecolors='none')
            
            if fine_data['iters']:
                ax.scatter(fine_data['iters'], fine_data['times'], 
                          label='Fine', color='red', alpha=0.6, s=15, marker='s', edgecolors='none')
            
            # è®¾ç½®yè½´ä¸Šé™ä¸º98åˆ†ä½æ•°ï¼Œé¿å…æç«¯å€¼å‹ç¼©æ•°æ®
            y_limit = np.percentile(all_times_array, 98) * 1.15
            ax.set_ylim(0, y_limit)
            
            # å¦‚æœæœ‰è¶…å‡ºèŒƒå›´çš„æç«¯å€¼ï¼Œæ·»åŠ æ ‡æ³¨
            outliers_count = np.sum(all_times_array > y_limit)
            if outliers_count > 0:
                ax.text(0.98, 0.98, f'âš ï¸ {outliers_count} outliers\n(max: {max_val:.3f}s)', 
                       transform=ax.transAxes, fontsize=8, 
                       verticalalignment='top', horizontalalignment='right',
                       bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
            
            # æ·»åŠ ä¸­ä½æ•°å‚è€ƒçº¿
            ax.axhline(y=median_val, color='green', linestyle='--', alpha=0.3, linewidth=1)
            ax.text(0.02, median_val, f'median: {median_val:.3f}s', 
                   fontsize=7, color='green', verticalalignment='bottom')
            
            ax.set_xlabel(f'Iteration (Coarse: 1-{coarse_max_iter}, Fine: {coarse_max_iter+1}-17000)', fontsize=10)
            ax.set_ylabel('Time (seconds)', fontsize=10)
            ax.set_title(f'{op.replace("_", " ").title()} Time Trend', 
                        fontsize=11, fontweight='bold')
            ax.legend(fontsize=9, loc='upper left')
            ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š æ“ä½œè¶‹åŠ¿å›¾å·²ä¿å­˜: {save_path}")
        
        plt.show()
    
    def plot_heatmap(self, save_path=None):
        """ç»˜åˆ¶æ“ä½œç”¨æ—¶çƒ­åŠ›å›¾ï¼ˆä¼˜åŒ–æ€§èƒ½ï¼šé‡‡æ ·æ•°æ®ç‚¹ï¼Œä¸æ˜¾ç¤ºæ–‡å­—æ ‡æ³¨ï¼‰"""
        data = self.extract_operation_data()
        
        # å‡†å¤‡çƒ­åŠ›å›¾æ•°æ®
        operations = []
        iterations = data['iterations']
        
        # æ”¶é›†æ‰€æœ‰æ“ä½œ
        for op in data['operations'].keys():
            if not op.endswith('_total'):
                operations.append(op)
        
        # åˆ›å»ºæ•°æ®çŸ©é˜µ
        matrix = []
        for op in operations:
            times = data['operations'][op]
            matrix.append(times)
        
        if not matrix:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°æ“ä½œæ•°æ®")
            return
        
        matrix = np.array(matrix)
        
        # æ€§èƒ½ä¼˜åŒ–ï¼šå¦‚æœiterationæ•°é‡è¿‡å¤šï¼ˆ>100ï¼‰ï¼Œè¿›è¡Œé‡‡æ ·
        max_iterations_display = 100
        if len(iterations) > max_iterations_display:
            # å‡åŒ€é‡‡æ ·
            sample_indices = np.linspace(0, len(iterations)-1, max_iterations_display, dtype=int)
            iterations_sampled = [iterations[i] for i in sample_indices]
            matrix = matrix[:, sample_indices]
            print(f"âš¡ çƒ­åŠ›å›¾é‡‡æ ·ä¼˜åŒ–ï¼šä» {len(iterations)} ä¸ªiterationsé‡‡æ ·åˆ° {max_iterations_display} ä¸ª")
        else:
            iterations_sampled = iterations
        
        # åˆ›å»ºçƒ­åŠ›å›¾
        fig, ax = plt.subplots(figsize=(14, 8))
        
        # ç»˜åˆ¶çƒ­åŠ›å›¾ï¼ˆä¸æ·»åŠ æ–‡å­—æ ‡æ³¨ä»¥æå‡æ€§èƒ½ï¼‰
        if HAS_SEABORN:
            sns.heatmap(matrix, 
                       xticklabels=[str(it) for it in iterations_sampled][::max(1, len(iterations_sampled)//20)],
                       yticklabels=[op.replace('_', ' ').title() for op in operations],
                       cmap='YlOrRd',
                       cbar_kws={'label': 'Time (seconds)'},
                       ax=ax,
                       annot=False)  # ä¸æ˜¾ç¤ºæ•°å€¼ï¼Œæå‡æ€§èƒ½
        else:
            # ä½¿ç”¨matplotlibç»˜åˆ¶çƒ­åŠ›å›¾
            im = ax.imshow(matrix, cmap='YlOrRd', aspect='auto', interpolation='nearest')
            
            # åªæ˜¾ç¤ºéƒ¨åˆ†xè½´æ ‡ç­¾ï¼Œé¿å…æ‹¥æŒ¤
            tick_step = max(1, len(iterations_sampled) // 20)
            ax.set_xticks(range(0, len(iterations_sampled), tick_step))
            ax.set_xticklabels([str(iterations_sampled[i]) for i in range(0, len(iterations_sampled), tick_step)], 
                              fontsize=8, rotation=45)
            
            ax.set_yticks(range(len(operations)))
            ax.set_yticklabels([op.replace('_', ' ').title() for op in operations], fontsize=9)
            
            # æ·»åŠ é¢œè‰²æ¡
            cbar = plt.colorbar(im, ax=ax)
            cbar.set_label('Time (seconds)', fontsize=10)
            
            # ä¸æ·»åŠ æ•°å€¼æ ‡æ³¨ï¼Œæå‡æ€§èƒ½
        
        ax.set_title('Operation Time Heatmap (Hover for details)', fontsize=13, fontweight='bold')
        ax.set_xlabel('Iteration (sampled for performance)', fontsize=11)
        ax.set_ylabel('Operation Type', fontsize=11)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š çƒ­åŠ›å›¾å·²ä¿å­˜: {save_path}")
        
        plt.show()
    
    def generate_summary_report(self):
        """ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š"""
        data = self.extract_iteration_data()
        op_data = self.extract_operation_data()
        
        print("\n" + "="*60)
        print("ğŸ“Š 4DGSè®­ç»ƒæ—¶é—´åˆ†ææŠ¥å‘Š")
        print("="*60)
        
        # åŸºæœ¬ç»Ÿè®¡
        total_training_time = self.data.get('total_training_time', 0)
        print(f"æ€»è®­ç»ƒæ—¶é—´: {total_training_time:.2f} ç§’ ({total_training_time/60:.1f} åˆ†é’Ÿ)")
        
        if data['iterations']:
            avg_time = np.mean(data['total_times'])
            min_time = np.min(data['total_times'])
            max_time = np.max(data['total_times'])
            print(f"å¹³å‡æ¯è½®ç”¨æ—¶: {avg_time:.3f} ç§’")
            print(f"æœ€å¿«è½®æ¬¡: {min_time:.3f} ç§’")
            print(f"æœ€æ…¢è½®æ¬¡: {max_time:.3f} ç§’")
            print(f"æ€»è½®æ¬¡æ•°: {len(data['iterations'])}")
        
        # å„æ“ä½œå¹³å‡ç”¨æ—¶
        print("\nå„æ“ä½œå¹³å‡ç”¨æ—¶:")
        for op, times in op_data['operations'].items():
            if times and any(t > 0 for t in times):
                avg_time = np.mean([t for t in times if t > 0])
                print(f"  {op.replace('_', ' ').title()}: {avg_time:.3f} ç§’")
        
        # è®­ç»ƒæŒ‡æ ‡ï¼ˆè¿‡æ»¤Noneå€¼ï¼‰
        valid_losses = [l for l in data['losses'] if l is not None and l > 0]
        if valid_losses:
            final_loss = valid_losses[-1]
            print(f"\næœ€ç»ˆæŸå¤±: {final_loss:.6f}")
        
        valid_psnrs = [p for p in data['psnrs'] if p is not None and p > 0]
        if valid_psnrs:
            final_psnr = valid_psnrs[-1]
            print(f"æœ€ç»ˆPSNR: {final_psnr:.2f}")
        
        print("="*60)
    
    def run_full_analysis(self, output_dir=None):
        """è¿è¡Œå®Œæ•´åˆ†æï¼ˆå¸¦æ€§èƒ½ç›‘æ§ï¼ŒV3.0ç‰ˆæœ¬ï¼‰"""
        import time
        
        if output_dir:
            output_dir = Path(output_dir)
            output_dir.mkdir(exist_ok=True)
        
        print("ğŸš€ å¼€å§‹4DGSè®­ç»ƒæ—¶é—´å¯è§†åŒ–åˆ†æ (V3.0)...")
        
        # ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š
        self.generate_summary_report()
        
        # ç”Ÿæˆå„ç§å›¾è¡¨å¹¶è®¡æ—¶
        chart_times = {}
        
        print("\nç”Ÿæˆå›¾è¡¨ä¸­...")
        
        # Coarse vs Fineé˜¶æ®µæ—¶é—´å¯¹æ¯”
        start = time.time()
        self.plot_stage_time_comparison(
            save_path=output_dir / "stage_time_comparison.png" if output_dir else None
        )
        chart_times['é˜¶æ®µæ—¶é—´å¯¹æ¯”'] = time.time() - start
        print(f"  â±ï¸  é˜¶æ®µæ—¶é—´å¯¹æ¯”: {chart_times['é˜¶æ®µæ—¶é—´å¯¹æ¯”']:.2f}ç§’")
        
        # è¿­ä»£æ—¶é—´æ›²çº¿ï¼ˆä¸¤ä¸ªç‰ˆæœ¬ï¼‰
        start = time.time()
        self.plot_iteration_timing_curve(
            save_path=output_dir / "iteration_timing_curve.png" if output_dir else None,
            save_path_full=output_dir / "iteration_timing_curve_full.png" if output_dir else None
        )
        chart_times['è¿­ä»£æ—¶é—´æ›²çº¿ï¼ˆä¸¤ä¸ªç‰ˆæœ¬ï¼‰'] = time.time() - start
        print(f"  â±ï¸  è¿­ä»£æ—¶é—´æ›²çº¿ï¼ˆä¸¤ä¸ªç‰ˆæœ¬ï¼‰: {chart_times['è¿­ä»£æ—¶é—´æ›²çº¿ï¼ˆä¸¤ä¸ªç‰ˆæœ¬ï¼‰']:.2f}ç§’")
        
        # æ“ä½œå æ¯”å›¾
        start = time.time()
        self.plot_operation_breakdown(
            save_path=output_dir / "operation_breakdown.png" if output_dir else None
        )
        chart_times['æ“ä½œå æ¯”å›¾'] = time.time() - start
        print(f"  â±ï¸  æ“ä½œå æ¯”å›¾: {chart_times['æ“ä½œå æ¯”å›¾']:.2f}ç§’")
        
        # æ“ä½œè¶‹åŠ¿å›¾
        start = time.time()
        self.plot_operation_trends(
            save_path=output_dir / "operation_trends.png" if output_dir else None
        )
        chart_times['æ“ä½œè¶‹åŠ¿å›¾'] = time.time() - start
        print(f"  â±ï¸  æ“ä½œè¶‹åŠ¿å›¾: {chart_times['æ“ä½œè¶‹åŠ¿å›¾']:.2f}ç§’")
        
        # å»æ‰çƒ­åŠ›å›¾ï¼ˆç”¨æˆ·åé¦ˆæ²¡ä»€ä¹ˆç”¨ï¼‰
        # self.plot_heatmap(...) - å·²ç§»é™¤
        
        total_time = sum(chart_times.values())
        print(f"\nâœ… åˆ†æå®Œæˆï¼æ€»è€—æ—¶: {total_time:.2f}ç§’")
        print(f"ğŸ“Š ç”Ÿæˆå›¾è¡¨ï¼š")
        print(f"  - stage_time_comparison.png (Coarse vs Fineæ—¶é—´å æ¯”)")
        print(f"  - iteration_timing_curve.png (98åˆ†ä½æ•°è§†å›¾)")
        print(f"  - iteration_timing_curve_full.png (æ–­è½´å®Œæ•´è§†å›¾)")
        print(f"  - operation_breakdown.png (æ“ä½œæ—¶é—´å æ¯”)")
        print(f"  - operation_trends.png (æ“ä½œè¶‹åŠ¿ï¼Œæ¨ªè½´å·²ä¿®æ­£)")
        print(f"\nğŸ“ æ”¹è¿›è¯´æ˜ï¼š")
        print(f"  âœ… æ¨ªè½´è¿ç»­æ€§ï¼šFineé˜¶æ®µæ¥åœ¨Coarseåï¼ˆ3000+ï¼‰")
        print(f"  âœ… æ–­è½´è§†å›¾ï¼šé—´è·å·²å¢åŠ ï¼Œæ¨ªåæ ‡ä¸å†é‡å ")
        print(f"  âœ… æ–°å¢é˜¶æ®µå¯¹æ¯”é¥¼å›¾")

def main():
    parser = argparse.ArgumentParser(description='4DGSè®­ç»ƒæ—¶é—´å¯è§†åŒ–åˆ†æå·¥å…·')
    parser.add_argument('timing_report', help='æ—¶é—´æŠ¥å‘ŠJSONæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', '-o', help='è¾“å‡ºç›®å½•ï¼ˆå¯é€‰ï¼‰')
    parser.add_argument('--curve', action='store_true', help='åªæ˜¾ç¤ºç”¨æ—¶æ›²çº¿')
    parser.add_argument('--breakdown', action='store_true', help='åªæ˜¾ç¤ºæ“ä½œå æ¯”')
    parser.add_argument('--trends', action='store_true', help='åªæ˜¾ç¤ºæ“ä½œè¶‹åŠ¿')
    parser.add_argument('--heatmap', action='store_true', help='åªæ˜¾ç¤ºçƒ­åŠ›å›¾')
    
    args = parser.parse_args()
    
    # åˆ›å»ºå¯è§†åŒ–å™¨
    visualizer = TimingVisualizer(args.timing_report)
    
    # æ ¹æ®å‚æ•°é€‰æ‹©æ˜¾ç¤ºå†…å®¹
    if args.curve:
        visualizer.plot_iteration_timing_curve()
    elif args.breakdown:
        visualizer.plot_operation_breakdown()
    elif args.trends:
        visualizer.plot_operation_trends()
    elif args.heatmap:
        visualizer.plot_heatmap()
    else:
        # æ˜¾ç¤ºæ‰€æœ‰å›¾è¡¨
        visualizer.run_full_analysis(args.output)

if __name__ == "__main__":
    main()
